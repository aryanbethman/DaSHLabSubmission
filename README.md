# DaSH-Lab-Assignment-2024

## Develompental Assignment

I have used gemma's huggingface API, which downloads pre-trained models via the transformers module. 

The development assignment is more or less complete, however further improvements could have been made if there was more time.
They are as follows:
1. I used the default pipeline with the gpt-2 model. The results could have been improved drastically had I used the             AutoTokenizer module and along with it, using PyTorch.
2. The response from the LLM could have been processed so that the response doesn't end mid sentence.
3. I used the "text-generator" mode for the pipeline. Maybe there was a more suitable one, but I couldn't find any.
4. I could not implement the UNIX timestamps.
5. I only implemented one client for the server.

My learnings with this assignment:
1. Learnt how to use the transformers module
2. Learnt about the various pipelines and how the data is processed in them
3. Learnt about client-server architecture and implemented it in python

## Project Assignment

Although I couldn't complete as much as I would like to have completed, I began reading a couple of research papers that were recommended about:
1. Segment-Anything-Model
2. Federated Learning Systems
3. LlamaFS


